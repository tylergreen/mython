#! /usr/bin/env
# ______________________________________________________________________
"""Module LL1Parser

Silly base class for LL1 parsers generated by pgen2LL1.

Jonathan Riehl

$Id: LL1Parser.py 61 2007-08-24 00:59:43Z jriehl $
"""
# ______________________________________________________________________
# Module imports

# ______________________________________________________________________
# Module definitions

__DEBUG__ = False

# ______________________________________________________________________
# Class definition

class LL1Parser (object):
    """Class LL1Parser
    """
    # ____________________________________________________________
    def __init__ (self, tokenizer, filename = None):
        """LL1Parser.__init__()
        """
        self.tokenizer = tokenizer
        self.next_token = None
        self.filename = filename
        self.stack = []
        self.keywords = []
        self.tok_names = {}
        self.line_offset = 0

    # ____________________________________________________________
    def __call__ (self, start_symbol = None):
        """LL1Parser.__call__()
        """
        if start_symbol is None:
            start_symbol = "start"
        parse_method = getattr(self, "parse_%s" % start_symbol)
        return parse_method()

    # ____________________________________________________________
    def tokenize (self):
        """LL1Parser.next_token()
        """
        return self.tokenizer.next()

    # ____________________________________________________________
    def get_token (self):
        """LL1Parser.get_token()
        """
        ret_val = None
        if self.next_token is None:
            ret_val = self.tokenize()
        else:
            ret_val = self.next_token
            self.next_token = None
        return ret_val

    # ____________________________________________________________
    def get_lookahead (self):
        """LL1Parser.get_lookahead()
        """
        ret_val = None
        if self.next_token is None:
            ret_val = self.tokenize()
            self.next_token = ret_val
        else:
            ret_val = self.next_token
        return ret_val

    # ____________________________________________________________
    def test_lookahead (self, *tokens):
        """LL1Parser.test_lookahead()
        """
        ret_val = False
        lookahead = self.get_lookahead()
        if (lookahead[0] in tokens) and (lookahead[1] not in self.keywords):
            ret_val = True
        elif lookahead[1] in tokens:
            ret_val = True
        return ret_val

    # ____________________________________________________________
    def push (self, data):
        """LL1Parser.push()
        """
        ret_val = (data, [])
        if self.stack:
            self.stack[-1][1].append(ret_val)
        self.stack.append(ret_val)
        return ret_val

    # ____________________________________________________________
    def pop (self):
        """LL1Parser.pop()
        """
        ret_val = self.stack[-1]
        del self.stack[-1]
        return ret_val

    # ____________________________________________________________
    def set_line_offset (self, lines):
        self.line_offset = lines

    # ____________________________________________________________
    def get_location_string (self, token):
        lineno = token[2][0] + self.line_offset
        if self.filename:
            ret_val = "File %r, line %d," % (self.filename, lineno)
        else:
            ret_val = "Line %d," % (lineno,)
        return ret_val

    # ____________________________________________________________
    def _token_type_to_str (self, tok_type):
        if tok_type in self.tok_names:
            ret_val = "%s token" % self.tok_names[tok_type]
        else:
            ret_val = "token type %d" % tok_type
        return ret_val

    # ____________________________________________________________
    def expect (self, token):
        """LL1Parser.expect()
        """
        crnt_token = self.get_token()
        if (crnt_token[0] != token) and (crnt_token[1] != token):
            if __DEBUG__:
                import pprint
                pprint.pprint(self.stack)
            locstr = self.get_location_string(crnt_token)
            if type(token) == int:
                desired_token_type = self._token_type_to_str(token)
                actual_token_type = self._token_type_to_str(crnt_token[0])
                err_str = ("%s expected %s, got %s (%r)." %
                              (locstr, desired_token_type, actual_token_type,
                               crnt_token))
                raise SyntaxError(err_str)
            else:
                err_format = "%s expected '%s', got '%s'(%s)."
                raise SyntaxError(err_format % (
                    locstr, token, crnt_token[1], str(crnt_token)))
        self.push(crnt_token)
        return self.pop()

# ______________________________________________________________________
# Function definition

def parser_main (parser_klass = None):
    import sys, tokenize, pprint
    # Tokenize
    if len(sys.argv) > 1:
        readline = open(sys.argv[1]).readline
    else:
        readline = sys.stdin.readline
    tokenizer = tokenize.generate_tokens(readline)
    # Parserize
    if parser_klass is None:
        parser_klass = LL1Parser
    parser = parser_klass(tokenizer)
    pprint.pprint(parser())

# ______________________________________________________________________

if __name__ == "__main__":
    parser_main()

# ______________________________________________________________________
# End of LL1Parser.py
