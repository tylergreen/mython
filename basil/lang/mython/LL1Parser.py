#! /usr/bin/env
# ______________________________________________________________________
"""Module LL1Parser

Silly base class for LL1 parsers generated by pgen2LL1.

Jonathan Riehl

$Id: LL1Parser.py 61 2007-08-24 00:59:43Z jriehl $
"""
# ______________________________________________________________________
# Module imports

# ______________________________________________________________________
# Module definitions

__DEBUG__ = False

# ______________________________________________________________________
# Class definition

class LL1Parser (object):
    """Class LL1Parser
    """
    # ____________________________________________________________
    def __init__ (self, tokenizer, filename = None):
        """LL1Parser.__init__()
        """
        self.tokenizer = tokenizer
        self.next_token = None
        self.filename = None
        self.stack = []
        self.keywords = []

    # ____________________________________________________________
    def __call__ (self, start_symbol = None):
        """LL1Parser.__call__()
        """
        if start_symbol is None:
            start_symbol = "start"
        parse_method = getattr(self, "parse_%s" % start_symbol)
        return parse_method()

    # ____________________________________________________________
    def tokenize (self):
        """LL1Parser.next_token()
        """
        return self.tokenizer.next()

    # ____________________________________________________________
    def get_token (self):
        """LL1Parser.get_token()
        """
        ret_val = None
        if self.next_token is None:
            ret_val = self.tokenize()
        else:
            ret_val = self.next_token
            self.next_token = None
        return ret_val

    # ____________________________________________________________
    def get_lookahead (self):
        """LL1Parser.get_lookahead()
        """
        ret_val = None
        if self.next_token is None:
            ret_val = self.tokenize()
            self.next_token = ret_val
        else:
            ret_val = self.next_token
        return ret_val

    # ____________________________________________________________
    def test_lookahead (self, *tokens):
        """LL1Parser.test_lookahead()
        """
        ret_val = False
        lookahead = self.get_lookahead()
        if (lookahead[0] in tokens) and (lookahead[1] not in self.keywords):
            ret_val = True
        elif lookahead[1] in tokens:
            ret_val = True
        return ret_val

    # ____________________________________________________________
    def push (self, data):
        """LL1Parser.push()
        """
        ret_val = (data, [])
        if self.stack:
            self.stack[-1][1].append(ret_val)
        self.stack.append(ret_val)
        return ret_val

    # ____________________________________________________________
    def pop (self):
        """LL1Parser.pop()
        """
        ret_val = self.stack[-1]
        del self.stack[-1]
        return ret_val

    # ____________________________________________________________
    def expect (self, token):
        """LL1Parser.expect()
        """
        crnt_token = self.get_token()
        if (crnt_token[0] != token) and (crnt_token[1] != token):
            if __DEBUG__:
                import pprint
                pprint.pprint(self.stack)
            if type(token) == int:
                # TODO, translate from token type to a string name.
                err_format = "Line %d, expected token type %d, got '%s'(%s)."
                raise SyntaxError(err_format % (crnt_token[2][0], token,
                                                crnt_token[1],
                                                str(crnt_token)))
            else:
                err_format = "Line %d, expected '%s', got '%s'(%s)."
                raise SyntaxError(err_format % (crnt_token[2][0], token,
                                                crnt_token[1],
                                                str(crnt_token)))
        self.push(crnt_token)
        return self.pop()

# ______________________________________________________________________
# Function definition

def parser_main (parser_klass = None):
    import sys, tokenize, pprint
    # Tokenize
    if len(sys.argv) > 1:
        readline = open(sys.argv[1]).readline
    else:
        readline = sys.stdin.readline
    tokenizer = tokenize.generate_tokens(readline)
    # Parserize
    if parser_klass is None:
        parser_klass = LL1Parser
    parser = parser_klass(tokenizer)
    pprint.pprint(parser())

# ______________________________________________________________________

if __name__ == "__main__":
    parser_main()

# ______________________________________________________________________
# End of LL1Parser.py
