% ______________________________________________________________________
\documentclass{article}

% $Id: thesis.tex 10042 2005-03-21 23:32:41Z jriehl $

\usepackage {epsfig}
%\usepackage{color, graphicx}

\author{Jonathan Riehl}
\title{Grammar Based Unit Testing for Parsers}
\begin{document}
% \maketitle
\begin{center}
{\huge Grammar Based Unit Testing for Parsers}

\vspace{.2in}

{\Large Master's Thesis}

\vspace{.2in}

{\Large Jonathan Riehl\\
University of Chicago\\
Department of Computer Science}

\vspace{.2in}

{\Large Abstract}
\end{center}
% ______________________________________________________________________

White box unit testing is the use of test inputs derived from source
code.  While approaches based on control flow analysis exist for
generating unit test inputs, these approaches are not readily applied
to codes that embed control flow as static data.  One specific example
of such code is the state machine built by parser generators.  Control
flow in these machine abstractions is based on static data that
encodes a set of state transition rules.  However, these rules also
correspond to the recognition of productions in the grammar input to
the parser generator.  It is therefore feasible to develop white box
tests for a parser based on an analysis of the input grammar.

This thesis will present a methodology for testing parsers based on
analysis of the grammars used to build the parsers.  Furthermore, this
paper will describe a tool set that assists in automation of test
generation.  This tool set uses a language grammar to create
grammatical strings in that language.  It will be shown how the set of
strings in a language may be navigated in a principled fashion for the
purpose of spanning the set of all language productions, and
heuristics developed for resolving nonterminal symbols into strings of
terminal symbols.  This paper will also demonstrate how these tools
were used to test a simple parser for the C programming language.

\begin{center}
Advisor: David Beazley
\end{center}
\newpage

% ______________________________________________________________________
\section {Introduction}

%\begin{figure}
%\begin{center}
%\epsfig{file=test.xfig.eps}
%\input{test.latex}
%\end{center}
%\caption{Jon Sux0rz}
%\label{fig:maximal-suckitude}
%\end{figure}

This section will introduce the topics and ideas that led to the unit
testing methodology and tools developed in this thesis.
Subsection~\ref{sec:white-box-testing} will illustrate traditional
white box testing methods.  Subsection~\ref{sec:parsers} will discuss
the operation of parsers and parser generators.  It will then show how
the code generated by a common parser generator complicates control
flow analysis, making white box testing of the parser impractical.
This section will conclude with an overview of generative grammars,
which will play a key role in the proposed solution to the problem of
unit testing parsers.

% ____________________________________________________________
\subsection {White Box Versus Black Box Testing}
\label{sec:white-box-testing}

Software testing activities may be organized according to a dichotomy based
on how test inputs to a program (or subprogram) are developed.  Under
this division, there are black box tests and white box tests.  Black
box tests are developed without consideration of a software's source
code.  In contrast, white box tests are developed using full knowledge
of the program's source code.

Black box tests are commonly guided by requirements.  For this reason,
the aim of many black box tests is to ensure that a subset of the
program requirements are met.  Since requirements describe program
functionality, much black box testing is also known as functional
testing.  One metric of black box test completeness is requirement
coverage.  Requirement coverage is a useful metric in traditional
software development processes, where the requirements are meant to be
decoupled from design and implementation.  However, it should be noted
that given the definition presented for black box tests, many unit
tests, or tests where only parts of programs are tested, are designed
to ensure a subprogram provides some base line functionality.  These unit
tests are not created based on analysis of unit control
flow, and do not employ the methodology described for white box
testing.

As previously discussed, white box testing is a form of software
testing where tests are developed based on structural analysis of the
source code.  The goal of a set of white box tests is to exercise all
control flow paths in a program.  Such rigor is employed in order to
gain a wider characterization of possible subprogram behavior than is
typically exposed by functional testing alone.  Where black box
testing makes sure code does what it is supposed to do, white box
testing helps to ensure that code does not do what it is not supposed
to do.  Other methods for gaining characterizations of program failure
may also include stress testing or testing boundary conditions
specified in (or implied by) a design contract.  However, only the
methodology behind white box testing makes certain guarantees about
the amount of code run during testing.

The finest metric of white box test completeness is full coverage of
all possible control flow paths in a program.  However, testing on
this scale is impractical even for small programs since the number of
paths in a program is exponential in the number of control flow
branches.  Correspondingly, white box tests may be measured using a
spectrum of metrics based on how closely the tester is required to
approximate full path coverage.

\begin{figure}
\begin{center}
\begin{tt}
\begin{tabular}{|r|l|}
\hline
1 & function foo (a, b, c): \\
2 & \quad if a AND b: \\
3 & \quad \quad bar() \\
4 & \quad if c: \\
5 & \quad \quad baz() \\
\hline
\end{tabular}
\end{tt}
\caption{Example code showing a function under test.}
\label{fig:example-control-flow-code}
\end{center}
\end{figure}

The coarsest metric of white box test completeness is based on the
number of lines of code run in test.  Achieving full line coverage
gives the guarantee that every line of code was run under test.  One
example where the value of line coverage diminishes is given in
Figure~\ref{fig:example-control-flow-code}.  In the function
\texttt{foo()}, only one test would be needed to achieve full line
coverage, but this does not consider the case where the call to the
\texttt{bar()} function may have side effects on program state that
impact the operation of \texttt{baz()} (or even the evaluation of the
conditional predicate.)  Since no else case is given in the example
program, line coverage does not require the tester to look at what
happens to the case when a conditional evaluates to false.

A more refined measure of white box test completeness is conditional
coverage.  Conditional coverage is a measure of the number of true and
false pairs run in test for each conditional in the source.  Testing
at this level of granularity would remedy the fact that testing for
full line coverage fails to simulate what happens when a conditional
branch is not taken.

A specific deficiency of testing for conditional coverage is
caused by the fact that many programming languages use semantics that
short circuit conditional truth evaluation.  Using these short circuit
semantics, the example source code will never evaluate the truth of
the $b$ predicate in cases where the $a$ predicate is already
evaluated as being false.  This can be somewhat remedied by increasing
coverage granularity to the level of logical predicates in a
conditional.  This presents a third level of coverage, one where metrics
measure the number of true and false pairs run for
each conditional predicate.

\begin{table}
\begin{center}
\begin{tabular}{|r|p{2in}|p{2in}|}
\hline
Level & Target & Example Test Inputs (a,b,c) \\
\hline
1 & Full line coverage & (1,1,1) \\
\hline
2 & Full conditional coverage & \emph{(1,1,1),} (1,0,0) \\
\hline
3 & Full predicate coverage &
\emph{(1,1,1), (1,0,0),} (0,1,1) \newline
(or also \emph{(1,1,1),} (0,0,0)) \\
\hline
4 & All predicate permutations &
\emph{(1,1,1), (1,0,1), (0,1,1),} (0,0,1),
(1,1,0),
(1,0,0),
(0,1,0),
(0,0,0) \\
\hline
\end{tabular}
\caption{Example test input at various levels of white box test granularity.}
\label{example-test-inputs}
\end{center}
\end{table}

Table~\ref{example-test-inputs} illustrates the coverage hierarchy
described, with the addition of a fourth level where all logical
permutations of predicates are considered.  Each level of coverage is
followed by example test inputs that would achieve the given coverage
goal when input to the \texttt{foo()} function in
Figure~\ref{fig:example-control-flow-code}.
Table~\ref{example-test-inputs} also shows how each level of coverage
may be looked at as a refinement of the previous level, emphasizing
the test inputs that were inherited from the inputs of the previous
coverage level.

Typical white box test methodology involves the selection of a level
of coverage.  A tester will then analyze features of the target
subprogram's control flow at the appropriate granularity.  Based on
the analysis, the tester will then determine input vectors that will
provide the desired coverage.  These input vectors are then fed
to a subprogram that has been instrumented to generate coverage data,
and the coverage results are then verified against the coverage target
of each test input.

% ____________________________________________________________
\subsection {Parsers and Parser Generators}
\label{sec:parsers}

% * Parsers are a typically a component of an interpreter or compiler.
% * Parsers perform syntactic analysis on a vector of input tokens.
% * First and foremost, parsers ensure that input token stream is
% syntactically correct under some language grammar.
% * The language
% * Parsers are often used to generate a syntax tree

A parser is typically a component of an interpreter or compiler and
is responsible for performing syntactic analysis on a vector of input
tokens.  All parsers will ensure that the input token vector
conforms to the syntax of a language.  Parsers are also commonly used
to build a syntax tree or some other structured model of the input
vector that is used as input to another component of the interpreter
or compiler.  While the syntax of a language can be left as implicit in the
implementation of the parser, a more common approach is to create a
context-free grammar specification for the language \cite{asu}.

A context-free grammar is useful since it may be used as input to a
parser generator.  A parser generator will create a parser using some
form of context-free grammar input, saving time by creating all
necessary logic and control flow for the parser.  Some
parser generators generate recursive descent parsers, where parser
control flow follows the language syntax.  However, a sizeable number
of parser generators are based on the \texttt{yacc} tool.

The \texttt{yacc} tool and its successors generate code that simulates
a set of state machines.  The state machines generated by
\texttt{yacc} will commonly employ state transition code that is
already well understood and tested.  The code generated for two input
grammars will differ in three ways: the parse table data, the reduction
actions, and header and footer code.  The parser table data is static
data commonly consisting of arrays of integer data.  The reduction
actions are code segments that are embedded in the input grammar, and
are intended to be run when the state machine detects a substring that
matches a grammar production.  Header and footer code is simply copied
from the parser generator input file to the front and end of the
generated parser code.

Application of white box unit test methodology to the output of a
parser generator is complicated by the fact that control flow is
determined in part by the state transition table.  As mentioned above,
the state transition table is presented in the parser source as an
array of integer data.  Performing control flow analysis of the parser
code will only grant insight into the operation of the state machine
excitation logic, not the operation of the parser itself.  While the
header and footer code may be isolated and tested using white box test
methods, these methods do not readily provide a means of targeting the
reduction actions without isolating the action case code from the
excitation logic.  This suggests an approach based on analysis of the
reductions in the input grammar would provide some means of targeting
parser actions.

% ____________________________________________________________
\subsection {String Generation Using Grammars}
\label{sec:string-gen}

Using a context free grammar it is possible to generate strings in the
language described by that grammar.  This section will first describe
how grammars are structured.  It will then show how to induce two
kinds of graphs on the structure of a grammar.  Both kinds of graphs have
applications for string generation, and will be used in algorithms
described later in this paper.

An arbitrary context-free grammar, $G$, may be described as a four
tuple, as shown below \cite{koz}:

$$G = (N, \Sigma, P, S)$$
$$N = \textrm{finite set of \emph{nonterminal symbols} in } G.$$
$$\Sigma = \textrm{finite set of \emph{terminal symbols} in } G.$$
$$P = \textrm{finite subset of } N \times ( N \cup \Sigma )*
\textrm{(the set of \emph{productions}.)}$$
$$S \in P \quad (= \textrm{the start symbol.})$$

A context-free grammar may be used to generate strings using
derivations.  The set of derivations is described by first defining
the following binary relation \cite{asu}:

\begin{displaymath}
s_0 \Rightarrow s_1 \iff (s_0 = \alpha_0 A \alpha_1)(s_1 = \alpha_0
\beta \alpha_1)((A, \beta) \in P)
\end{displaymath}

A derivation is a sequence of strings, $s_0 \Rightarrow s_1
\Rightarrow \ldots \Rightarrow s_n$, such that $s_i \in (N \cup
\Sigma)*$ and $s_i \Rightarrow s_{i+1}$.  The common
abbreviation $s_0 \Rightarrow^{*} s_n$ is used to imply that there
exists a derivation from $s_0$ to $s_n$, where $\Rightarrow^{*}$ is
the transitive closure of the $\Rightarrow$ relation.

\subsubsection{String Graphs}

Using the definition of the single step derivation relation, a
directed graph structure, $Gr(G)$, may be defined:

$$Gr(G) = (V(G), E(G))$$
$$V(G) = (N \cup \Sigma)*$$
$$E(G) = \{ (v_0, v_1) | v_0 \Rightarrow v_1 \}$$

%(\exists A \in N) (v_0 = \alpha_0 A \alpha_1) (\exists \beta \in (N
%\times \Sigma)*) ( (a, \beta) \in P) (v_1 = \alpha_2 \beta \alpha_3)

The strings in the language $L(G)$ may be defined as being the set of
vertices in $Gr(G)$ that have an out degree of zero and are reachable from
$S$.  Generating strings in $L(G)$ can therefore be accomplished by performing
walks on $Gr(G)$ from $S$ to strings that are in $\Sigma *$.  These
walks correspond to derivations, by definition of $E(G)$.

\subsubsection{Production Graphs}
\label{sec:production-graph}

Another useful graph may be induced on the productions in a grammar:

$$PG(G) = (PV(G), PE(G))$$
$$PV(G) = \{p_0\} \cup \{p_i | p_i \in P\}$$
$$PE_0(G) = \{ (p_0, p_i) | (p_i = (S, \gamma)) \}$$
$$PE_1(G) = \{ (p_i, p_j) | (\exists A \in N)(p_i = (B, \alpha_0 A \alpha_1))
(p_j = (A, \beta)) \}$$
$$PE(G) = PE_0(G) \cup PE_1(G) $$

Later sections will refer to $PG(G)$ as the production graph for
grammar $G$.  Walks in $PG(G)$ also correspond to derivations in $G$,
but are constrained such that the next single step derivation replaces
only nonterminal symbols inserted by the preceding single step
derivation.  The vertex $p_0$ is added to provide a starting point for
generating walks of $PG(G)$ that correspond to derivations starting
with the grammar's start symbol.

% ______________________________________________________________________
\section {Methodology}
\label{sec:methodology}

Section~\ref{sec:white-box-testing} showed how coverage
metrics drive the kind of code analysis required to generate white
box test inputs.  By switching the metric, a methodology for white box
testing automatically generated parsers becomes simple to develop.
In Section~\ref{sec:parsers} it was suggested that coverage of
productions in the grammar was a useful metric.  The following
methodology is based on this observation, employing production
coverage as one measure of parser white box test completeness.

% ____________________________________________________________
\subsection {A Parser Testing Procedure}
\label{sec:parser-test-procedure}

By using productions as a coverage metric, the process of white box
testing a parser reduces to identifying a target production, creating
a test string that will cause the target production to be applied, and
verifying that the target action handling code was run in the test.  Once
a means of running the action handling code has been identified,
coverage of the action handler code must still meet target coverage
levels.  Therefore, multiple test inputs may target the same
production.  In this case, it is left to the tester to determine a
means of obtaining full coverage of the action code.  When all
productions have been fully exercised, testing is complete.

%In Section~\ref{sec:string-gen} it was shown how given an input
%grammar, $G$, strings in the language described by $G$ could be
%automatically found by performing walks in a graph structure.
%Constraints on this technique may be applied to generate test strings
%in $L(G)$ that are known to exercise specific productions in $G$.
%Therefore, the process of generating test inputs that target a
%specific production may be automated.

% This methodology is not designed to test the correctness of the
% state machine generated by the parser generator.

% This methodology is not designed to test the error handling
% capabilities of the generated parser, and does not generate
% strings not in the language.

The following two step method has been developed to automatically
generate unit test strings.  The first step involves identification of
a string in $(N \cup \Sigma)*$ that is guaranteed to exercise some
production in $P$.  The second step involves a walk of $Gr(G)$
starting at the identified string and ending at a string of all
terminal symbols.  The resulting string is then admitted to the set of
white box tests for grammar $G$.  The following two subsections detail
each step in this process, and identify improvements to the algorithms
sketched above.

% ____________________________________________________________
\subsection {Automation of Production Coverage}
\label{sec:auto-production}

The first step in creation of test strings is to generate a string in
$(N \cup \Sigma)*$ that exercises some production in $P$.  An initial
implementation of this step employed a constraint based breadth first
search (CBFS) of $Gr(G)$.  The constraints input to the CBFS algorithm
were of the form ``the derivation from $S$ to the current vertex $v$
contains production, $p_i \in P$.''  Using these constraints, CBFS
converges under the assumption that the input grammar does not
contain productions that are unreachable from the start symbol.
However, a more straightforward solution is to use the production
graph, $PG(G)$.  Using a minimum spanning tree (MST) algorithm that
starts from vertex $p_0$ in $PG(G)$, all reachable productions will be
covered by the derivations corresponding to the walks from $p_0$ to
the leaf vertices in the resulting tree \cite{pur}.  Since the MST
algorithm converges, the test string generator would as well, even
when $PG(G)$ is not fully connected.

Section~\ref{sec:parser-test-procedure} noted that a testing approach
based on single production coverage does not begin to address the
issue of control flow in the production action code.  Control flow in
parser action code is typically a function of a state that is
determined by the prior sequence of productions already recognized by
the parser.  By changing the coverage metric to the measure of
possible unordered pairs of productions, the automation begins to
assist the tester by looking at the side effects one action may have
on another.  Here the set of constraints for searching $Gr(G)$ becomes
``vertex $n$ such that $n$ is reached by some application of both
production $p_i$ and $p_j$'' for each unordered pair of productions,
$(p_i, p_j), p_i, p_j \in P$, such that there exists a derivation, $S
\Rightarrow^{\star} n$, that employs production $p_i$ and $p_j$.
Since up to $|P|(|P| - 1)/2$ additional strings are generated,
the expanded set of strings may also be seen as providing a set of
stress tests on the parser.

\begin{figure}
\begin{center}
$$\begin{array}{|r|l|}
\hline
1 & S \rightarrow A \\
2 & S \rightarrow B \\
3 & A \rightarrow a \\
4 & B \rightarrow b B \\
5 & B \rightarrow c \\
6 & S \rightarrow SS \\
\hline
\end{array}$$
\caption{An example context-free grammar.}
\label{fig:example-grammar}
\end{center}
\end{figure}

This refinement of the coverage metric is not without problems.
First, it ignores the fact that inter-production interactions in the
parser state are almost certainly asymmetric.  This asymmetry
implies that \emph{ordered} pairs of productions are a better
coverage metric.  However, the order of productions in a
derivation does not necessarily correspond to the order in which
productions are recognized by a parser.  This paper does not consider
the added complexity of adding inputs to the string generation process
that would enable correlation between a string derivation order and
the expected production recognition order.  A second issue with using
an unordered production pair metric is it increases the likelihood of
a CBFS algorithm diverging. Some input grammars may define mutually
exclusive productions causing some of the input constraints to never
be met.  For example, removing the sixth production from the grammar shown in
Figure~\ref{fig:example-grammar} would cause the first and second
productions to be mutually exclusive.  The second issue is avoided by
using a constructive algorithm that navigates the production graph
instead of using a constraint based search of $Gr(G)$.

The constructive string generation algorithm, detailed in
Appendix~\ref{app:string-gen-alg}, works by identifying three cases of
production pairs.  The first case occurs when two productions are
covered by a string already found using the leaves of the MST of
$PG(G)$.  The algorithm begins by building the minimum spanning tree
for $PG(G)$, starting from $p_0$.  When two productions appear on the
same route to a leaf vertex in the spanning tree, they are marked as
being covered by the string derived from the leaf vertex.  The second
set of pairs cover the case where a path exists from one production to
another in $PG(G)$.  To find these cases, the algorithm builds a
spanning tree for each production.  For each spanning tree, the
algorithm marks a production pair for each pairing of the root
production with a production in the spanning tree.  The final case
covers some pair $(p_i, p_j)$ when the string derived by a walk from
$p_0$ to $p_i$ contains a nonterminal that can reach $p_j$.  The
string generation algorithm begins by using the spanning trees in the
prior step to determine which productions are reachable by each of the
nonterminal symbols in the grammar.  The algorithm finds the remaining
pairs by iterating over the set of unique walks from $p_0$ to $p_i$,
deriving a string for each walk, and then marking $(p_i, p_j)$ as
covered if the string contains a nonterminal that reaches $p_j$.

\begin{figure}
\begin{center}
\epsfig{file=ProductionGraph.xfig.eps}
\end{center}
\caption{Production graph for the grammar in Figure~\ref{fig:example-grammar}.}
\label{fig:example-production-graph}
\end{figure}

Some examples of allowable pairings for the grammar in
Figure~\ref{fig:example-grammar} are: $(p_3, p_1)$, $(p_5, p_4)$, and
$(p_3, p_4)$.  These tuples represent an example of one
of the three cases of pairings.  The first pair, $(p_3, p_1)$, is an
example of a tuple found using the spanning tree of $PG(G)$.
Figure~\ref{fig:example-production-graph} shows the production graph
for the example grammar.  The spanning tree for the graph has the
following leaves: $p_3$, $p_6$, $p_4$, and $p_5$.  Since $p_1$
precedes $p_3$ in the spanning tree, the derivation corresponding to
the walk from $p_0$ to $p_3$ covers both $p_1$ and $p_3$.  The second
pair, $(p_5, p_4)$, results from building a spanning tree in $PG(G)$
rooted at $p_5$.  The final tuple, $(p_3, p_4)$, is found by noting
that the walk $(p_0, p_6, p_1, p_3)$ leaves a spare $S$ nonterminal in
the derived string.  Since $S$ reaches $p_4$ by way of $p_2$, the
derivation corresponding to application of productions 6, 1, 3, 2, and
4 covers the pair $(p_3, p_4)$.

% ____________________________________________________________
\subsection {Resolution of Nonterminal Symbols to Strings of Terminals}
\label{sec:nonterms-to-strings}

Once a set of strings in $(N \cup \Sigma)*$ are found, it remains to
resolve these strings into strings of all terminal symbols, $\Sigma*$.
One solution would be to iterate over each string, performing some
search in $Gr(G)$ starting at the string, and ending at the first
vertex in $Gr(G)$ that has no out degree (by definition of $E(G)$,
these are vertices that correspond to strings in $\Sigma*$.)  This
approach is inefficient because each walk in $Gr(G)$ could follow the
same set of productions for common nonterminals in the strings.

It is sufficient to build a map from the set of nonterminals, $N$, to
a subset of strings in $\Sigma*$.  Such a map may be built by
iterating over the set of nonterminal symbols, $N$.  For each
nonterminal symbol, $A$, some search over $Gr(G)$ is performed.  The
search would start at the vertex corresponding to the string
consisting only of the nonterminal, $A$, and ending at some vertex
with an out degree of zero.  Further time may be saved by memoization
of prior search results, replacing nonterminals in a working string
with the substring corresponding to the result of the search for the
known nonterminal symbol.

During development of the initial prototype, a breadth first search
(BFS) algorithm was initially used for the search algorithm.  However,
this consumed all system memory for the C++ grammar under
consideration.  The grammar presented in
Figure~\ref{fig:example-grammar} presents an example of why a depth
first search (DFS) will also fail for most input grammars.  Assuming
The DFS algorithm is employed, recursion in the input grammar may
cause DFS to diverge.  Therefore some sort of heuristic search is
required.

The heuristic search is similar in form to the BFS algorithm, but for
each vertex a cost is calculated.  When a vertex is removed from the
frontier, only vertices of minimal cost are chosen.  By using the
length of the symbol string associated with each vertex as a measure
of cost, the heuristic search is capable of finding a map from
nonterminal symbols to strings of all terminal symbols in a reasonable
amount of time and within an appropriate space bound.

% ______________________________________________________________________
\section {Design and Implementation}

The test string generation algorithm outlined in
Section~\ref{sec:methodology} is intended to assist language
implementors perform white box tests on parsers they develop.  It was
therefore deemed beneficial to integrate the test string generator
into a programming language development framework.  This section will
provide a high level description of the Basil language framework, show
how the test string generation application fits into the software
framework, and will demonstrate how the application was used to test
another component of the framework.

% ____________________________________________________________
\subsection {The Basil Language Framework}

The Basil language framework is a Python \cite{van} based set of libraries
and applications that are intended to assist software developers in
two ways.  First, the framework is meant to provide a common
infrastructure for programming language implementation.  Second, the
framework should provide reusable components that allow the
development of cross language software engineering and analysis tools.
These goals are met by providing components that are customized to
support development of specific portions of the compiler pipeline.

The framework is organized into four sets of components.  Components
that are built around a target data representation are called
integrations.  There are three sets of integrations: parser generator
integrations, language integrations, and model integrations.  The
fourth set of components are applications.  The set of Basil
applications is a catch-all for tools present in the framework that
employ the functionality of one or more integrations, but are not
deemed as being a part of a specific integration.

% __________________________________________________
\subsubsection {Parser Generator Integrations}

A parser generator integration is a component that translates from the
input language of some parser generator to a grammar model.  A parser
generator integration may also provide some means for translation of a
grammar model back to a parser generator input file, but this is not a
requirement.  A parser generator integration will typically consist of
a parser for the parser generator input, a translation back end that
walks a syntax tree of the input file and generates a grammar model,
and an optional model walker that walks a grammar model instance and
generates a input source for the target parser generator.  Examples of
parser generators that are targeted for integration are yacc and
Python's parser generator, pgen.  Parser generator integrations
may also re-implement or couple more tightly to parser generators to
assist with the development of interactive grammar development tools.

% __________________________________________________
\subsubsection {Language Integrations}
\label{sec:lang-integrations}

A language integration is essentially a parser that translates a
source string to a syntax tree for some predefined language.  Language
integrations target a syntax tree model that employs native Python
data structures for speed purposes.  This may be contrasted to other
model integrations which use objects to represent model instances.  A
standard model integration incurs time and space overhead issues
because of the large number of model elements present in a parse tree.
Language integrations may also include facilities to generate source
code, but in many cases it is simpler to output code as text to the
console or a source file, versus writing a tree walker to output code.

% __________________________________________________
\subsubsection {Model Integrations}

A model integration either defines or is generated from a data
structure description, referred to as a meta-model.  The meta-model is
exposed as a set of classes and a factory class.  The factory class is
responsible for model serialization and deserialization, as well as
exposing model element construction methods.  The goal of a model is
to serve as an in-memory representation of some intermediate language.
Some examples of target model domains would be object models (MOF,
UML), linkage interface models (SWIG), control flow graphs, data flow
graphs, and the Basil grammar model.

The use of class based model representations allows meta-model
development to employ third party object oriented modeling tools
\cite{qua} \cite{rob}.  Class based models also allow models to contain
cyclic references, structuring data as directed graphs.  This may be
contrasted with hierarchical data organizations such as in-memory
representations of XML, which force a model developer to
employ indirect references to other model elements.  However, as
was mentioned in Section~\ref{sec:lang-integrations}, the overhead of
object construction and navigation can be expensive, and Basil also
provides a tuple based facility for creating, storing and manipulating
tree data structures.

%Model integrations may
%also expose language translation interfaces.  constructing models based
%on the syntax tree generated by a specific language integration.  

% __________________________________________________
\subsubsection {Applications}

Applications are framework components that will typically provide
translation facilities from one model domain to another.  One example
application would be a component that walks a grammar model instance
and outputs a model integration, thus creating a parse tree model
specific to the language specified in the grammar model.  By employing
an arbitrary parser generator integration to build the grammar model
instance, the application would allow model integrations to be
automatically built for all inputs for all supported parser
generators.  In cases where an application is useful in the
development of the Basil framework, it will often be exposed as a
program that may be run from the OS shell.

% ____________________________________________________________
\subsection {The Basil Test String Generation Application}

\begin{figure}
\begin{center}
\epsfig{width=1.0\textwidth, file=ApplicationDataflow.xfig.eps}
\end{center}
\caption{Dataflow of the Test String Generation Application}
\label{fig:test-app}
\end{figure}

Figure~\ref{fig:test-app} illustrates the components and data products
that comprise the bulk of the test string generation application.
Also shown is an example of how to use the command line interface.
The command line interface uses the \texttt{handle.py} program.  The
\texttt{handle.py} program is a Basil application that provides the
glue for building compilers in Basil.  Application back ends are run
from \texttt{handle.py} by specification of an intermediate language
(as served by a model integration), the application back end (also
termed a \emph{handler} of the input model), and an input source file.

When a model factory is asked to parse an input file and construct a
new model instance, the factory will dispatch to a parser or front end
based on the file extension given in the input file name.  For the
test string generator, the Basil grammar model is employed.  When
\texttt{handle.py} is run, the grammar model dispatches to a
parser/handler pair.  The selected parser will build a parse tree that
is then input to the handler.  The handler will walk the parse tree, and
create a new grammar model instance.  Finally the new model instance
is fed to the top level handler.

The top level handler for the test string generator implements the
algorithms described in Section~\ref{sec:methodology}.  The generator
begins operation by extracting the set of productions in the input
grammar and a start symbol (the first nonterminal symbol listed in the
model is used if none is present.)  Using the start symbol and the set
of productions, the production graph is constructed and used to build
test strings.  A map from all nonterminal symbols to strings of
terminal symbols is generated using a heuristic search of $Gr(G)$.
During the heuristic search only parts of the infinite di-graph,
$Gr(G)$, are instantiated.  The resulting map is then applied to any
nonterminals found in the strings generated in the first part.  The
resulting test strings are then output to the terminal.  Optionally,
the set of productions targeted by each string and the map from
nonterminal symbols to terminal strings may also be output.

% ____________________________________________________________
\subsection {Example Usage}
\label{sec:example}

%Here I will use gcov, and the test string generator to test the Basil
%C parser.  XXX - How to document this stuff?  I certainly don't want
%to just drop in a copy of my parser's yacc file, the test strings and
%the gcov results.

Using the test string generator, it becomes possible to develop white
box test strings for languages supported by the framework.  For
example, the C language integration uses a parser generated by Bison.
Since a Bison parser generator integration is available, the test
string generator may be used to generate test input for the C language
integration.  These strings are generated using the following command:

\begin{verbatim}
% handle.py models/grammar/BasilGrammarModel \
            models/grammar/Testerizer \
            lang/c/cparser.y -verbose > c-test-strings.txt
\end{verbatim}

The resulting strings generated are shown as follows:

\begin{verbatim}
0. CONST ';' CONST ';'
1. CONST ';' CONST ';' CONST ';'
2. CONST ';' IDENTIFIER '{' '}'
3. CONST ';' CONST ';'
4. IDENTIFIER CONST ';' '{' '}'
...
\end{verbatim}

The index numbers allow the tester to cross reference the target
production list with the generated string.  For example:

\begin{verbatim}
...
0. ('external_declaration', 'external_declaration')
[(0, ('translation_unit', ('translation_unit',
                           'external_declaration'))),
 (0, ('translation_unit', ('external_declaration',))),
 (0, ('external_declaration', ('function_definition',)))]
...
\end{verbatim}

The first tuple is the string as originally phrased in mixed
terminal and nonterminal symbols.  The list of tuples following the string
identifies the set of productions that are being targeted by that
string.  The tester may verify these productions were exercised by the
given string using a coverage analysis tool such as \texttt{gcov}.

In order to use these test cases the tester must map from nonterminal
symbol names to actual lexical entities.  The resulting strings must
then be input to an instrumented parser that allows verification of
coverage.  For example, the Basil C parser can be compiled as a
command line program that accepts C source from the console.  The
following describes how the test methodology was applied to the Basil
C parser.

\texttt{GCC} instrumented the command line C parser, as instructed to by
use of the ``\texttt{-fprofile-arcs}'' and
``\texttt{-ftest-coverage}'' arguments.  The strings output from the
string generator were copied to test input files.  The test input
files were then piped to the instrumented parser.  During parsing, the
parser instrumentation generated coverage files, which were analyzed
using \texttt{gcov}.

\begin{verbatim}
% make "CFLAGS=-g -fprofile-arcs -ftest-coverage" cparser
[ Builds an instrumented C parser ]
% cparser < tests/test6.0.in
[ Runs the instrumented C parser, 
  generating instrumentation data files ]
% gcov cparser.tab.c
[ Creates file cparser.y.gcov ]
\end{verbatim}

Since Bison generates source with the C \texttt{\#line} pragma,
\texttt{gcov} is able to trace coverage data directly back to the
original Bison input file (thus generating a ``cparser.y.gcov'' file
as opposed to ``cparser.tab.c.gcov'', which could be more difficult to
read.)  The lines of ``cparser.y.gcov'' that correspond to those
targeted by the selected test string are shown in
Figure~\ref{fig:gcov-output}.  The numbers in the left margin count
the number of times the given line was run by the instrumented parser.
Not shown in Figure~\ref{fig:gcov-output} are lines that were not run
in test.  Lines not executed in test would have a series of hash marks
in the left margin to mark their untested status.  In a full
test situation, the tester would continue by identifying productions
not tested, and apply the same methods shown above until all code was
shown to have run in test.

\begin{figure}
\begin{verbatim}
...
              translation_unit
                      : external_declaration
                      {
         1              $$ = $1;
         1              CParserSetRoot($1);
                      }
         1            | translation_unit external_declaration
              {
         2      $$ = CParserNewNode(TRANSLATION_UNIT, 2);
         2      CParserSetChild($$, 0, $1);
         2      CParserSetChild($$, 1, $2);
         2      CParserSetRoot($$);
              }
         2            ;
...
\end{verbatim}
\caption{Partial output of \texttt{gcov} for an instrumented run of the Basil
C Parser}
\label{fig:gcov-output}
\end{figure}

% map from terminal symbol names to actual lexical constructs.

% ______________________________________________________________________
\section {Related Work}

Various approaches to white box testing, also termed as structural
testing are described by Jorgensen \cite{jor}.  Specifically,
Jorgensen attributes development of coverage metrics to Edward F. Miller
\cite{mil}, who developed a lattice of coverage metrics similar
to those described in Section~\ref{sec:white-box-testing}.  Thomas McCabe
developed metrics and tools to support both flow control analysis and
coverage instrumentation of software \cite{wat}.  A more recent survey
of attempts to automate structural testing can be found in Edvardsson
\cite{edv}.  In his survey, Edvardsson mentions sytax driven test case
generation, but only to comment that is it outside the scope of the paper.

String generation from grammars begins with Chomsky's fomalization of
grammar, where he describes his formalism as being ``a device of some
sort for producing the sentences of the language under analysis''
\cite{cho}.  Grammar based string generation has been widely used in
the creation of procedural graphics.  Lindenmayer is well known for
using generative grammars as a means of describing and procedurally
generating plant structure \cite{pru}.  Generative grammars have also
been applied to generation of music \cite{ler}.  Peter Maurer's Data
Generation Language (DGL) generates strings using an input grammar,
which have been used in circuit testing \cite{mau}.

Most closely related to the work done here is the work of Paul Purdom
\cite{pur}.  In \cite{pur}, Purdom decomposes the problem of testing
parsers in a fashion similar to
Section~\ref{sec:parser-test-procedure}, where an algorithm is given
to find a mapping from nonterminal symbols to strings of terminal
symbols, and a second algorithm is used to find a set of derivations
that cover all productions in a grammar.  Purdom's high level methods
are geared toward verification of parser correctness, rather than
acting as a substitute for structural testing of a parser.  Another
important difference is Purdom's algorithm for mapping nonterminal
symbols to strings of terminals is much more efficient than the one
given in Section~\ref{sec:nonterms-to-strings}, and should be adopted
in a future release of Basil.  In \cite{lam}, L\"ammel extends
Purdom's techniques with a new metric based on coverage of all edges
in the production graph, $PG(G)$, as defined in
Section~\ref{sec:production-graph}.

Grammars appear again in the field of model checking, which is related
to functional testing.  In model checking, software is modeled as
automata while requirements are specified as logical assertions about
automata behavior.  Both the logical assertions about model
correctness and the automaton model can be reformulated as grammars.
A model checker may then test for intersection of the complement of
the assertion language and the automata language, and ensure the
intersection of the two languages is empty.  One such model checker is
SPIN, developed at Bell Labs by Gerard Holzmann \cite{hol}.  Research
into using source code to create model automata (thus moving model
checking closer to structural testing) is also being undertaken.
This requires expansion of the kind of admissible models, from finite
automata to a subclass of infinite automata \cite{alu1}, and logics
capable of making assertions about the expanded automata models
\cite{alu2}.

% L-Systems

% Model checking, formal methods and theorem proving - can integrate some
% research from recent talk
% (3/11/2004) that characterized models in terms of language grammar.

% ______________________________________________________________________
\section {Future Directions}

The demonstration of toolset usage provided in Section~\ref{sec:example}
illustrates points where automation may be further enhanced.  Ideally,
the loop of string generation and coverage verification could be
automated, but in a highly platform dependent fashion (for example,
scripting of the test run and \texttt{gcov} output validation would be
specific to parsers generated by Bison on a platform with
\texttt{gcov} and GCC.)  It would also be desirable to automate the
translation from terminal symbol names to actual lexical strings, even
if not all terminal symbols would not have a clear one-to-one mapping
(such as identifiers.)  Another step would be to allow some subset of
semantics to be associated with productions.  Therefore if the test
string generator emitted a C string with an identifier, the tester
could have the tool insert a declaration of the identifier before its
use.

The methodology as presented is focused on generation of strings in
some language, $L(G)$, for the purpose of driving coverage of source
code under some metric.  However, if the stated purpose of these
exercises is to ensure a parser/compiler \emph{does not do what it is
not supposed to do}, it is not a complete means of addressing common
failure cases inherrent in language implementation.  Therefore, a more
robust test methodology should look at not only how a parser behaves
given strings in the language but how well it recovers given strings
that are agramatical.  Agrammatical strings would be encompassed by
use of a metric that measured coverage of a production as well as
coverage of an error case for that production.  The generator would
accomplish this by misapplication of the production, such as
omitting or inserting a symbol.  One problem with this method is
the parsing algorithm used could complicate direct correlation between
an intended error in a production and the error actually exercising
error handling code for a production.

Furthermore, programming language test methodology should distinguish
between tests of the language that are grammatical, but are either
semantically invalid or semantically valid.  Language formalisms that
take the form of context sensitive grammars may assist with
development of not only a superset of the methodology presented, but
also extension of the test automation.  Under the adapted methodology,
the target coverage metric would measure counts of both violation and
correct usage of all semantic productions under test.  Generation of
these inputs would make the automation more like an automated theorem
generator, and such tools would be able to employ common techniques
used in theorem discovery.

% Still want citations for Felleisen, possibly Visser, and theorem stuff.

% ______________________________________________________________________
\section {Conclusion}

White box testing of parsers is complicated by the nature of common
parser implementation techniques.  Parsers that use state machines
will have control flow hidden from standard code analysis techniques.
Instead of language level control flow constructs, the generated
parser encodes state transitions in lookup tables, and state actions
in flat case statements.  However, the structure of the state machine
follows the structure of the language it is intended to recognize, and
this fact may be exploited through analysis of the language's grammar.
By modification of the coverage metric considered by the white box
test methodology, control flow is abstracted to focus on recognition
of productions in the grammar.  At this level of abstraction, tests
focus on excercising the language implementor's code, and not the
generic state machine code developed for use by a parser generator.

By using a language's grammar as a generative grammar, automation of
the modified methodology becomes feasible.  Search techniques and a
useful heuristic have been illustrated.  The algorithms have been
employed in an application that assists in the development white box
test cases for a language parser.  It is hoped this application will
be of particular value in the context of a programming language
prototyping framework, allowing domain specific languages to not only
be rapidly developed, but rapidly tested as well.

% ______________________________________________________________________
%\section {Works Cited}

\begin{thebibliography}{99}

% http://www.cis.upenn.edu/~madhusud/stoc04.ps
\bibitem{alu1}  Alur, Rajeev, and P. Madhusudan.  \emph{Visibly Pushdown
Languages}.  STOC 04.  Chicago, 2004.  Association for Computing
Machinery, \emph{to appear}.

% http://www.cis.upenn.edu/~madhusud/tacas04.ps
\bibitem{alu2}  Alur, Rajeev, Kousha Etessami, and P. Madhusudan.
\emph{A Temporal Logic of Nested Calls and Returns}.  TACAS 2004.
Barcelona, Spain, 2004.  Heidelberg: Springer-Verlag, 2004.

\bibitem{asu} Aho, Alfred V., Ravi Sethi, and Jeffrey D. Ullman.
\emph{Compilers: Principles, Techniques, and Tools}   Reading, Mass.:
Addison-Wesley Publishing Company, 1986.

\bibitem{cho} Chomsky, Noam. \emph{Syntactic Structures}  The Hague:
Mouton, 1976.

% class_atdg.pdf
\bibitem{edv} Edvardson, Jon.  ``A Survey on Automatic Test Data
Generation''  Proceedings of the Second Conference on Computer Science
and Engineering in Linköping, CCSSE'99, ProNova, Norrköping, 1999.

\bibitem{hol} Holzmann, Gerard.  ``The Model Checker SPIN.''
\emph{IEEE Transactions on Software Engineering} 23.5 (1997): 279-295.

\bibitem{jor} Jorgensen, Paul C.  \emph{Software Testing: A
Craftsman's Approach}. Boca Raton: CRC Press, 2002.

\bibitem{koz} Kozen, Dexter C.  \emph{Automata and Computability}  New
York: Springer-Verlag, 1997.

\bibitem{lam} L\"ammel, Ralf.  ``Grammar Testing.''  \emph{Proceedings
of Fundamental Approaches to Software Engineering}.  FASE 2001.  LNCS
v.2029.  New York: Springer-Verlag, 2001.

\bibitem{ler} Lerdahl, Fred, and Ray Jackendoff.  \emph{A Generative
Theory of Tonal Music}  Cambridge, Mass.: MIT Press, 1983.

\bibitem{mau} Maurer, Peter M.  ``The Design and Implementation of
a Grammar-based Data Generator.''  \emph{Software - Practice and
Experience} 22.3 (1992): 223-244.  

\bibitem{mil} Miller, Edward F. Jr. \emph{Tutorial: Program Testing
Techniques}.  COMPSAC 77.  Chicago, 1977.  IEEE Computer Society,
1977.

\bibitem{pru} Prusinkiewicz, P., and A. Lindenmeyer.  \emph{The
Algorithmic Beauty of Plants}  New York: Springer-Verlag, 1990.

\bibitem{pur} Purdom, P.  ``A Sentance Generator for Testing
Parsers.''  \emph{BIT} 22.3 (1972): 366-375.

\bibitem{qua} Quatrani, Terry.  \emph{Visual Modeling With Rational
Rose and UML}.  Reading, Mass.: Addison-Wesley Publishing Company, 1997.

\bibitem{rob} Robbins, J. E., D. Hilbert, and D. Redmiles.
``Extending Design Environments to Software Architecture Design.''
\emph{Automated Software Engineering: An International Journal} 5.3
(1998): 261-290.

\bibitem{van} van Rossum, Guido.  \emph{Python Reference Manual}
Ed. Fred L. Drake, Jr.  Python Labs, 2003.

\bibitem{wat} Watson, Arthur H., and Thomas McCabe. \emph{Structured
Testing : a Testing Methology Using the Cyclomatic Complexity Metric}.
Ed. Dellores Wallace.  Gaithersburg, Maryland: NIST, Special
Publication 500-235, 1996.

\end{thebibliography}

% ______________________________________________________________________

\newpage
\appendix
\section {The String Generation Algorithm}
\label{app:string-gen-alg}

The following section provides a pseudo-code listing of the string
generation algorithm outlined in Section~\ref{sec:auto-production}.
The pseudo-code follows Python syntax, most notibly using the Python
slicing operator to perform some list manipulation \cite{van}.  The
algorithm also relies on the following functions:

\begin{itemize}
\item Flatten: Tree $\rightarrow$ List of Vertex

Returns a list of the vertices reachable from the root vertex of the
input tree.

\item LeafWalks: Tree $\rightarrow$ List of Walk

Returns the walks from the root vertex of the tree to the leaf vertices.

\item Leaves: Tree $\rightarrow$ List of Vertex

Returns the leaf vertices in the input tree.

\item LeftDerivation: Walk $\rightarrow$ Vector of Symbol

Returns the string of symbols that is derived by the input walk
(assumes the walk is of a production graph), starting with the start
symbol.  Each application of a production in the walk is applied to
the leftmost nonterminal matching the left hand side of the
production.

\item LHS: Vertex $\rightarrow$ Symbol

Returns the the left hand side of the production represented by the
input vertex.

\item MST: Graph $\times$ Vertex $\rightarrow$ Tree

Returns a minimum spanning tree of the input graph using the input vertex as
a starting point.

\item SymmetricAssociation: (Map from (A $\times$ A) to B) $\times$ A
$\times$ A $\times$ B $\rightarrow$ (Map from (A $\times$ A) to B)

Sets both orderings of the A type inputs to reference the B type value
in the input map.

\item TreeWalk: Tree $\times$ Vertex $\rightarrow$ Walk

Returns the walk from the root of the input tree to the target vertex.

\item Walks: Graph $\times$ Vertex $\times$ Vertex $\rightarrow$ List of Walk

Returns a set of walks from the first vertex to the second vertex,
under the following contraint: Let E be the intersection of edges
reachable from the first vertex and the set of edges that can reach
the second vertex.  Every edge in E must be used by at least one walk
in the set of output walks.

\end{itemize}

%\begin{figure}[!hpb]
{\small
\begin{verbatim}
Inputs:
        G : Grammar
Outputs:
        TestStrings : List of List of Symbol
        TestMap : Map from (Vertex x Vertex) to Walk
Locals:
        BaseTree : Tree
        ReachableMap : Map from Symbol to List of Vertex
        ReachMap : Map from (Symbol x Vertex) to Walk

BaseTree = MST(PG(G), p_0)
TestStrings = Map(LeftDerivation, LeafWalks(BaseTree, p_0))
for p_i in (PV(G) - {p_0}):
    walk = TreeWalk(BaseTree, p_i)
    for p_j in walk[1:-1]:
        TestMap = SymmetricAssociation(TestMap, p_i, p_j, walk)

# Populate mapping for case 2 test cases.
for p_i in (PV(G) - {p_0}):
    tree = MST(PG(G), p_i)
    symbol_i = LHS(p_i)
    ReachableMap[symbol_i] = {}
    for p_j in Flatten(tree):
        if (p_j, p_i) not in TestMap:
            walk = TreeWalk(BaseTree, p_i) + TreeWalk(tree, p_j)
            TestMap = SymmetricAssociation(TestMap, p_j, p_i, walk)
            TestStrings = Union(TestStrings, LeftDerivation(walk))
        # Initialize data for case 3
        symbol_j = LHS(p_j)
        walk_i_to_j = First(Walks(tree, p_i, p_j))
        if symbol_j not in ReachableMap[symbol_i]:
            ReachableMap[symbol_i] = Union(ReachableMap[symbol_i],
                                           {p_j})
            if len(walk_i_to_j) < len(ReachMap[(symbol_i, p_j)]):
                ReachMap[(symbol_i, p_j)] = walk_i_to_j

# Populate mapping for case 3 test cases.
for p_i in (P - {p_0}):
    for walk in Walks(PG(G), p_0, p_i):
        passed = False
        derivation = LeftDerivation(walk[:-1])
        for symbol in (derivation - {LHS(p_i)}):
            for p_j in ReachableMap[symbol]:
                if (p_j, p_i) not in TestMap:
                    crntWalk = walk + ReachMap[(symbol, p_j)]
                    TestMap = SymmetricAssociation(TestMap, p_j,
                                                   p_i, crntWalk)
                    TestStrings = Union(TestStrings,
                                        LeftDerivation(crntWalk))
\end{verbatim}}

% ______________________________________________________________________
\end{document}
